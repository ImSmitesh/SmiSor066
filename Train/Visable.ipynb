{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_pDcitARSXcT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lrmvJ47fSfm_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"sample_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yq84_GsJSfzo"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V1wR-3J-SnTi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "#lower casing the data\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "#Converting text to vectors\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "vectorizer_filename = \"tfidf_vectorizer.pkl\"\n",
        "joblib.dump(vectorizer, vectorizer_filename)\n",
        "\n",
        "#Encoding the labels to neumeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['label'])\n",
        "encoder_filename = \"l_encoder_vectorizer.pkl\"\n",
        "joblib.dump(label_encoder, encoder_filename)\n",
        "\n",
        "#Splitting the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvlyuvItSpOm",
        "outputId": "31f3c099-6177-46e9-f2b4-88fdb032c761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Logistic Regression: 0.8065600215082672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.71      0.73       706\n",
            "           1       0.84      0.72      0.77       513\n",
            "           2       0.92      0.76      0.83      1022\n",
            "           3       0.73      0.92      0.82      2281\n",
            "           4       0.85      0.70      0.77      1009\n",
            "           5       0.86      0.82      0.84      1908\n",
            "\n",
            "    accuracy                           0.81      7439\n",
            "   macro avg       0.83      0.77      0.79      7439\n",
            "weighted avg       0.82      0.81      0.81      7439\n",
            "\n",
            "==================================================\n",
            "Model saved as Logistic Regression_model.pkl\n",
            "Training SVM...\n",
            "Accuracy for SVM: 0.8204059685441591\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.69      0.76       706\n",
            "           1       0.85      0.78      0.81       513\n",
            "           2       0.92      0.77      0.84      1022\n",
            "           3       0.75      0.92      0.83      2281\n",
            "           4       0.88      0.71      0.78      1009\n",
            "           5       0.83      0.85      0.84      1908\n",
            "\n",
            "    accuracy                           0.82      7439\n",
            "   macro avg       0.85      0.79      0.81      7439\n",
            "weighted avg       0.83      0.82      0.82      7439\n",
            "\n",
            "==================================================\n",
            "Model saved as SVM_model.pkl\n",
            "Training Random Forest...\n",
            "Accuracy for Random Forest: 0.8206748218846619\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.70      0.74       706\n",
            "           1       0.85      0.77      0.81       513\n",
            "           2       0.92      0.76      0.83      1022\n",
            "           3       0.76      0.93      0.83      2281\n",
            "           4       0.87      0.70      0.78      1009\n",
            "           5       0.86      0.85      0.85      1908\n",
            "\n",
            "    accuracy                           0.82      7439\n",
            "   macro avg       0.84      0.78      0.81      7439\n",
            "weighted avg       0.83      0.82      0.82      7439\n",
            "\n",
            "==================================================\n",
            "Model saved as Random Forest_model.pkl\n",
            "Training Gradient Boosting...\n",
            "Accuracy for Gradient Boosting: 0.7017072187121925\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.52      0.67       706\n",
            "           1       0.87      0.74      0.80       513\n",
            "           2       0.98      0.53      0.69      1022\n",
            "           3       0.55      0.94      0.69      2281\n",
            "           4       0.90      0.52      0.66      1009\n",
            "           5       0.81      0.65      0.72      1908\n",
            "\n",
            "    accuracy                           0.70      7439\n",
            "   macro avg       0.84      0.65      0.71      7439\n",
            "weighted avg       0.78      0.70      0.70      7439\n",
            "\n",
            "==================================================\n",
            "Model saved as Gradient Boosting_model.pkl\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "\n",
        "# Defining different models to train\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'SVM': SVC(decision_function_shape='ovo'),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Training and evaluating each model\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Save the trained model using joblib\n",
        "    model_filename = f\"{name}_model.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model saved as {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ9Ac85ZYACe",
        "outputId": "2013153d-bb15-4122-8b96-93a290448552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mr'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Load your trained TF-IDF vectorizer\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# Load your trained model\n",
        "model = joblib.load('Gradient Boosting_model.pkl')\n",
        "\n",
        "# Get input text from the user\n",
        "text = input(\"Enter text: \")\n",
        "\n",
        "# Transform the input text using the fitted vectorizer\n",
        "new_text_features = vectorizer.transform([text])\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(new_text_features)\n",
        "encoder = joblib.load(\"l_encoder_vectorizer.pkl\")\n",
        "y_pred_text = encoder.inverse_transform(y_pred)\n",
        "y_pred_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "# Assuming you have already preprocessed your dataset and split it into train and validation sets\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define your training and validation datasets\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len=128)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len=128)\n",
        "\n",
        "# Define your BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            _, predicted_labels = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(predicted_labels == labels).item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
